# -*- coding: utf-8 -*-
"""IBM_HR_Analytics_Employee_Attrition_Modeling_Project_Self_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vOdUWBVy2eiavQ8SLdmIqzaf6Fze80dU
"""

# Commented out IPython magic to ensure Python compatibility.
#First, import the dictionary that we known
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import sklearn
import seaborn as sns
from patsy import dmatrices

#Read the .csv file
dataframe = pd.read_csv("/content/IBM Attrition Data.csv")

#See the each title on the column from the table
title = dataframe.columns.values
title

#Look the head of data
dataframe.head()

#Zoom in the figure use plt.figure
fig, axis = plt.subplots(figsize=(12,8))
#Make a plot or histogram for age
dataframe['Age'].hist(bins=100, color = 'blue', edgecolor ='black', linewidth=0.8)
plt.title("Age distribution of Employees")
plt.xlabel("Age")
plt.ylabel("Employees")
plt.show()

#Zoom in the figure use plt.figure
fig, axis = plt.subplots(figsize=(12,8))
#Make a plot or histogram for age
dataframe.Department.value_counts().plot(kind='barh', linewidth=0.8)
plt.title("Department")
plt.show()

#Explore attrition by age, we can use scatter plot
fig, axis = plt.subplots(figsize=(12,8))

#use scatter
plt.scatter(dataframe.Attrition, dataframe.Age)
plt.title("Attrition by age of the Employees")
plt.ylabel("Age")
plt.grid(axis='y')
plt.show()

#Explore data for Left employees
fig, axis = plt.subplots(figsize=(10,8))
dataframe.Attrition.value_counts().plot(color='blue', edgecolor='black', kind='barh')
plt.title("Attrition breakdown of the Employees")
plt.show()

#distribution of employees by the education field
fig, axis = plt.subplots(figsize=(10,8))
dataframe.EducationField.value_counts().plot(color='r', edgecolor='black', kind='barh')
plt.title("Education Field Distribution of Employees")
plt.show()

#Explore the marital status of the employees
fig, axis = plt.subplots(figsize=(12,8))
dataframe.MaritalStatus.value_counts().plot(color='g', kind='bar')
plt.title("Marital status of the Employees")
plt.show()

"""Build up a logistic regression model to predict which employees are likely to attrite."""

#See the detail of dataframe
dataframe.describe()

#Find out about info of each columne
dataframe.info()

#Change the type from Object to int64
dataframe['Attrition'].value_counts()

#Replace the binary option (yes or no) with binary number (1 and 0) to help us
dataframe['Attrition'].replace('Yes',1, inplace=True)
dataframe['Attrition'].replace('No',0, inplace=True)

#Check the data
dataframe.head(5)

#Now, we build up a logistic regression model
X = dataframe.drop(['Attrition'],axis=1)
X.head()
Y = dataframe['Attrition']
Y.head()

dataframe['EducationField'].value_counts()

dataframe['EducationField'].replace('Life Sciences',1, inplace=True)
dataframe['EducationField'].replace('Medical',2, inplace=True)
dataframe['EducationField'].replace('Marketing', 3, inplace=True)
dataframe['EducationField'].replace('Other',4, inplace=True)
dataframe['EducationField'].replace('Technical Degree',5, inplace=True)
dataframe['EducationField'].replace('Human Resources', 6, inplace=True)

dataframe['EducationField'].value_counts()

dataframe['Department'].value_counts()

dataframe['Department'].replace('Human Resources',3, inplace=True)
dataframe['Department'].replace('Sales',2, inplace=True)
dataframe['Department'].replace('Research & Development', 1, inplace=True)

dataframe['Department'].value_counts()

dataframe['MaritalStatus'].value_counts()

dataframe['MaritalStatus'].replace('Married',1, inplace=True)
dataframe['MaritalStatus'].replace('Single',2, inplace=True)
dataframe['MaritalStatus'].replace('Divorce', 3, inplace=True)

dataframe['MaritalStatus'].value_counts()

x=dataframe.select_dtypes(include=['int64'])
x.dtypes

x.columns

y = dataframe['Attrition']
y.head()

y, x = dmatrices('Attrition ~ Age + Department + DistanceFromHome + \
                  Education + EducationField + YearsAtCompany', dataframe, return_type='dataframe')

x.columns
y = np.ravel(y)

from sklearn.linear_model import LogisticRegression

mymodel = LogisticRegression()
mymodel = mymodel.fit(x, y)

#Check Accuracy
print(mymodel.score(x,y))

print(y.mean())

X_train,X_test,y_train,y_test=sklearn.model_selection.train_test_split(x,y, test_size=0.3, random_state=0)
mymodel2=LogisticRegression()
mymodel2.fit(X_train, y_train)

print(X_train.shape)
print(X_test.shape)

predicted = mymodel2.predict(X_test)
print(predicted)

#Determine the probability
probability = mymodel2.predict_proba(X_test)
print (probability)

from sklearn import metrics

print (metrics.accuracy_score(y_test, predicted))
print (metrics.roc_auc_score(y_test, probability[:, 1]))

print (metrics.confusion_matrix(y_test, predicted))
print (metrics.classification_report(y_test, predicted))

print (X_train)

#add random values to KK according to the parameters mentioned above to check the proabily of attrition of the employee
kk=[[1.0, 23.0, 1.0, 500.0, 3.0, 24.0, 1.0]]
print(mymodel.predict_proba(kk))

from sklearn.decomposition import PCA

sklearn_pca = PCA(n_components=0.95)

sklearn_pca.fit(X_train)

X_train_transformed = sklearn_pca.transform(X_train)
print(X_train_transformed.shape)

X_test_transformed = sklearn_pca.transform(X_test)
print(X_test_transformed)

mymodel2 = LogisticRegression(penalty='l2')

mymodel2.fit(X_train_transformed,y_train)

y_predict=mymodel2.predict(X_test_transformed)
print(y_predict)

from sklearn import metrics

print (metrics.accuracy_score(y_test, y_predict))